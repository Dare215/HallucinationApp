{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b187d993-d6b6-4fa1-8fef-fe8ddb8d03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample prompt-completion pairs categorized by hallucination type\n",
    "examples = [\n",
    "    # Factual Hallucination\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Who was the first U.S. president to walk on the moon?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No U.S. president has ever walked on the moon.\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Prompt: What year did Albert Einstein win the Nobel Prize in Chemistry?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Albert Einstein won the Nobel Prize in Physics in 1921, not in Chemistry.\\n\"\n",
    "    },\n",
    "\n",
    "    # Logical Chain Consistency\n",
    "    {\n",
    "        \"prompt\": \"Prompt: If all mammals have lungs and whales are mammals, do whales have lungs?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Yes, whales have lungs because they are mammals.\\n\"\n",
    "    },\n",
    "\n",
    "    # Contradiction\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Paris is the capital of France and also the capital of Germany. Is that true?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No, Paris is the capital of France. Berlin is the capital of Germany.\\n\"\n",
    "    },\n",
    "\n",
    "    # Temporal Reasoning\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Was Barack Obama president during the Great Depression?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No, the Great Depression occurred in the 1930s. Barack Obama was president from 2009 to 2017.\\n\"\n",
    "    },\n",
    "\n",
    "    # False Premise\n",
    "    {\n",
    "        \"prompt\": \"Prompt: How did Cleopatra use airplanes to travel across Egypt?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Cleopatra lived over 2,000 years ago, long before airplanes were invented.\\n\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df216a87-d76e-45f9-808e-8d178cd0ac50",
   "metadata": {},
   "source": [
    "Next, examples of Hallucinations will be saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521091df-54ad-4f96-bded-5b61cf945505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 examples saved to hallucination_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save the list to a JSONL file\n",
    "output_file = \"hallucination_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in examples:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"{len(examples)} examples saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb85cdee-f9cc-4091-9a61-9272c1b78f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 6 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `?\\n\\n###\\n\\n`\n",
      "- All prompts start with prefix `Prompt: `\n",
      "- All completions end with suffix `.\\n`\n",
      "\n",
      "No remediations found.\n",
      "\n",
      "You can use your file for fine-tuning:\n",
      "> openai api fine_tunes.create -t \"hallucination_dataset.jsonl\"\n",
      "\n",
      "After you’ve fine-tuned a model, remember that your prompt has to end with the indicator string `?\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.53 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f hallucination_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa07738-c906-4140-9fae-ed1a2190b177",
   "metadata": {},
   "source": [
    "    Then we will  install and import an Open AI SDK then connect it to the APi platform by OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c88fc3ab-3bf5-4387-8d7c-ec4de9e075c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "File ID: file-BwytE9Kx1NgdC7hBzuGshe\n",
      "SyncCursorPage[FileObject](data=[FileObject(id='file-BwytE9Kx1NgdC7hBzuGshe', bytes=1107, created_at=1753142756, filename='hallucination_dataset.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-Qtxcd5isZXdBHTXuSrGEe2', bytes=1107, created_at=1753141190, filename='hallucination_dataset.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)], has_more=False, object='list', first_id='file-BwytE9Kx1NgdC7hBzuGshe', last_id='file-Qtxcd5isZXdBHTXuSrGEe2')\n"
     ]
    }
   ],
   "source": [
    "# Install the updated OpenAI SDK\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import the updated client\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Create the client using your API key\n",
    "client = OpenAI(api_key=\"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\") \n",
    "\n",
    "# Upload your file\n",
    "with open(\"hallucination_dataset.jsonl\", \"rb\") as f:\n",
    "    file_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(\"File ID:\", file_response.id)\n",
    "\n",
    "# List all files\n",
    "file_list = client.files.list()\n",
    "print(file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdec09-a989-4389-9f02-8b767bc6e687",
   "metadata": {},
   "source": [
    "Now i will provide the model with examples of catergoerized examples of prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21aa7f6-31a5-42b7-9336-3117efa32a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_dataset_50.jsonl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Categories\n",
    "categories = {\n",
    "    \"Factual Hallucination\": [\n",
    "        (\"Who was the first U.S. president to walk on the moon?\", \" No U.S. president has ever walked on the moon.\\n\"),\n",
    "        (\"What year did Albert Einstein win the Nobel Prize in Chemistry?\", \" Albert Einstein won the Nobel Prize in Physics in 1921, not in Chemistry.\\n\"),\n",
    "        (\"What city is the capital of Atlantis?\", \" Atlantis is a mythical place and does not have a real capital.\\n\"),\n",
    "        (\"Who directed the 1890 movie Titanic?\", \" The movie Titanic was released in 1997 and was directed by James Cameron.\\n\"),\n",
    "        (\"Which planet in our solar system is made of chocolate?\", \" No planet in our solar system is made of chocolate.\\n\")\n",
    "    ],\n",
    "    \"Logical Chain\": [\n",
    "        (\"If all birds can fly and penguins are birds, can penguins fly?\", \" No, penguins are birds but cannot fly.\\n\"),\n",
    "        (\"If dogs are mammals and mammals have hearts, do dogs have hearts?\", \" Yes, dogs have hearts because they are mammals.\\n\"),\n",
    "        (\"If a square has four equal sides and a rectangle has four sides, is every rectangle a square?\", \" No, not every rectangle is a square.\\n\"),\n",
    "        (\"If all apples are fruits and all fruits are edible, are apples edible?\", \" Yes, apples are edible because they are fruits.\\n\"),\n",
    "        (\"If John is taller than Alice and Alice is taller than Bob, who is the tallest?\", \" John is the tallest.\\n\")\n",
    "    ],\n",
    "    \"Contradiction\": [\n",
    "        (\"Water is dry. Is this true?\", \" No, water is not dry.\\n\"),\n",
    "        (\"The sun is cold and dark. Is that accurate?\", \" No, the sun is hot and emits light.\\n\"),\n",
    "        (\"Cats are reptiles. Agree or disagree?\", \" Disagree. Cats are mammals, not reptiles.\\n\"),\n",
    "        (\"Fire is made of ice. Explain.\", \" That is incorrect. Fire and ice are opposite in nature.\\n\"),\n",
    "        (\"Birds have fur. Is that correct?\", \" No, birds have feathers, not fur.\\n\")\n",
    "    ],\n",
    "    \"Temporal Reasoning\": [\n",
    "        (\"Was Barack Obama president during the Civil War?\", \" No, the U.S. Civil War occurred in the 1860s. Barack Obama was president from 2009 to 2017.\\n\"),\n",
    "        (\"Did Leonardo da Vinci tweet his inventions?\", \" No, Twitter did not exist during da Vinci's time in the 15th and 16th centuries.\\n\"),\n",
    "        (\"Did humans walk on Mars in 1969?\", \" No, humans walked on the Moon in 1969, not Mars.\\n\"),\n",
    "        (\"Was the iPhone released before the invention of electricity?\", \" No, electricity predates the iPhone by centuries.\\n\"),\n",
    "        (\"Was Julius Caesar interviewed on television?\", \" No, Julius Caesar lived in ancient Rome, long before television was invented.\\n\")\n",
    "    ],\n",
    "    \"False Premise\": [\n",
    "        (\"How did Cleopatra use airplanes to travel across Egypt?\", \" Cleopatra lived over 2,000 years ago, long before airplanes were invented.\\n\"),\n",
    "        (\"What type of email did Abraham Lincoln use?\", \" Abraham Lincoln lived in the 1800s, before email existed.\\n\"),\n",
    "        (\"How many TikTok followers did Shakespeare have?\", \" Shakespeare lived in the 1500s, long before TikTok was created.\\n\"),\n",
    "        (\"What video games did Napoleon enjoy?\", \" Napoleon lived in the 18th century, long before video games were invented.\\n\"),\n",
    "        (\"How did Martin Luther King Jr. post on Instagram?\", \" Instagram did not exist during Martin Luther King Jr.'s lifetime.\\n\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Expand to 50 examples\n",
    "examples = []\n",
    "for category, qa_pairs in categories.items():\n",
    "    for i in range(10):  # Generate 10 prompts per category\n",
    "        prompt_text, answer_text = random.choice(qa_pairs)\n",
    "        prompt = f\"Prompt: {prompt_text}\\n\\n###\\n\\n\"\n",
    "        completion = answer_text\n",
    "        examples.append({\"prompt\": prompt, \"completion\": completion})\n",
    "\n",
    "# Save to JSONL\n",
    "output_file = \"hallucination_dataset_50.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in examples:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "output_file  # Return path to file for user download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e72cb2-afeb-448d-9776-c77765f7396c",
   "metadata": {},
   "source": [
    "--Upload json and prepare for for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d71032-98c0-4fa9-8363-a3ac4a151578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Uploaded File ID: file-MaYnataKJjZ2xhQ4cpT35u\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Upload Json\n",
    "with open(\"hallucination_dataset_50.jsonl\", \"rb\") as file:\n",
    "    upload_response = client.files.create(\n",
    "        file=file,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "file_id = upload_response.id  # Note: using .id instead of [\"id\"]\n",
    "print(\"Uploaded File ID:\", file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2c29a-3906-46f4-8bf2-26fcc57827fe",
   "metadata": {},
   "source": [
    "next a fine tunig job will be created to train the model on the different catergories to be tested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eb13c4e-1cae-4916-b658-baaebb8ffbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Fine-tune Job ID: ftjob-JvW2TQLpItNYQK3pO7lA83dd\n"
     ]
    }
   ],
   "source": [
    "# Make sure the library is installed\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import the client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client by passing your API key directly\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Create the fine-tuning job\n",
    "fine_tune_response = client.fine_tuning.jobs.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the job ID\n",
    "job_id = fine_tune_response.id\n",
    "print(\"Fine-tune Job ID:\", job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1106ce28-aa1d-4e80-b5a6-3d83dc50e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.11.0)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.97.0\n",
      "    Uninstalling openai-1.97.0:\n",
      "      Successfully uninstalled openai-1.97.0\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597ded81-1221-4295-8081-f0b4eb3ff0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: failed\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Authenticate\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Use your actual fine-tune job ID below\n",
    "job_id = \"ftjob-JvW2TQLpItNYQK3pO7lA83dd\"  \n",
    "\n",
    "# Track the fine-tuning job status\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(\"Job Status:\", response.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40e30d-fcab-4332-8e87-9c988b29cb10",
   "metadata": {},
   "source": [
    "Update syntax due to failed job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "807f1984-126e-481d-91fa-7a2893680484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job failed due to a file format error in the training file. Invalid file format. Example 1 is missing key \"messages\".\n",
      "Validating training file: file-MaYnataKJjZ2xhQ4cpT35u\n",
      "Created fine-tuning job: ftjob-JvW2TQLpItNYQK3pO7lA83dd\n"
     ]
    }
   ],
   "source": [
    "# The correct syntax likely requires passing the job_id directly as a positional argument\n",
    "# or using a different parameter name according to the updated API\n",
    "events = client.fine_tuning.jobs.list_events(\"ftjob-JvW2TQLpItNYQK3pO7lA83dd\", limit=20)\n",
    "# Alternatively, if the API structure has changed:\n",
    "# events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-JvW2TQLpItNYQK3pO7lA83dd\", limit=20)\n",
    "\n",
    "for e in events.data:\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feb8dc04-fdfd-47b8-ac3b-2dfcec1efc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_chat_50.jsonl'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create 50 varied, correctly structured chat-format examples\n",
    "chat_examples = []\n",
    "for i in range(1, 51):\n",
    "    chat_examples.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that detects hallucinations and ensures logical consistency.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Example {i}: Can I time travel using a microwave and some magnets?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"No, that is a hallucination. Time travel is not possible with current technology, especially not using household items.\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Save all 50 as a JSONL file\n",
    "file_path_50 = \"hallucination_chat_50.jsonl\"\n",
    "with open(file_path_50, \"w\") as f:\n",
    "    for entry in chat_examples:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "file_path_50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca792cc-d560-41ed-b95d-3108a95f1b94",
   "metadata": {},
   "source": [
    "Re-prep the model for fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "045a89be-8e1a-4e5a-a61c-34141444ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Using cached openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.97.0\n",
      "Uploaded File ID: file-UYsRw5D6f2RjeUUtdshdHj\n"
     ]
    }
   ],
   "source": [
    "# Install the latest OpenAI package (if not already)\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import and configure the OpenAI client (v1.x+ syntax)\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Upload your fine-tuning file\n",
    "with open(\"hallucination_chat_50.jsonl\", \"rb\") as f:\n",
    "    upload_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "# Get and display the uploaded file ID\n",
    "file_id = upload_response.id\n",
    "print(\"Uploaded File ID:\", file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba6b9c1c-2ac9-4b47-b2bb-8307405a9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8966837-0c8f-43f0-bde1-76654c0a6b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-DtRs1pqLScMHccUYif6hacT2\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Create fine-tuning job\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-MaYnataKJjZ2xhQ4cpT35u\",  \n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Output the Job ID\n",
    "print(\"Fine-tuning Job ID:\", fine_tune_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b63f67-334c-4c87-b953-716a61a7d888",
   "metadata": {},
   "source": [
    "Next we will track the  perfomance and retest for a succesful test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c9e36ca-85c8-4d0d-8601-0fe355240608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: failed\n"
     ]
    }
   ],
   "source": [
    "# Track job status\n",
    "job_id = \"ftjob-Y8NL2MHx3YKB9xcShJVITb1o\"  \n",
    "\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(\"Job Status:\", job_status.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cce55-30ac-4208-ba01-eaae6a3b49a7",
   "metadata": {},
   "source": [
    "We will relist the fine tuning events due tp the version of the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f754e87c-c44e-4e24-a49c-6b8fadd1079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753147028 - The job failed due to a file format error in the training file. Invalid file format. Example 1 is missing key \"messages\".\n",
      "1753147026 - Validating training file: file-MaYnataKJjZ2xhQ4cpT35u\n",
      "1753147026 - Created fine-tuning job: ftjob-Y8NL2MHx3YKB9xcShJVITb1o\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual job ID\n",
    "job_id = \"ftjob-Y8NL2MHx3YKB9xcShJVITb1o\"\n",
    "\n",
    "# Correct way to list events for a fine-tuning job\n",
    "events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
    "# OR depending on the API version:\n",
    "# events = client.fine_tuning.jobs.list_events(id=job_id)\n",
    "\n",
    "for event in events.data:\n",
    "    # Use dot notation instead of dictionary-style access\n",
    "    # The FineTuningJobEvent object has properties that can be accessed with dot notation\n",
    "    print(f\"{event.created_at} - {event.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c077a57-9555-4dbf-85a8-46fd51fe85bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_chat_50_fixed.jsonl'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Generate 50 properly formatted chat-style examples\n",
    "examples = []\n",
    "for i in range(1, 51):\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that avoids hallucinations and ensures logical consistency.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question {i}: What are some ways to prevent AI from hallucinating responses?\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Answer {i}: AI hallucinations can be reduced by grounding the model in verified data, using retrieval-augmented generation, and fine-tuning on domain-specific examples.\"}\n",
    "        ]\n",
    "    }\n",
    "    examples.append(example)\n",
    "\n",
    "# Save as JSONL file\n",
    "jsonl_path = \"hallucination_chat_50_fixed.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "jsonl_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b424459-2774-4ec0-933b-e33a3cf592b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded File ID: file-WBomPEap6Dgg6N4hmQ6wn3\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "# Upload the corrected file\n",
    "with open(\"hallucination_chat_50_fixed.jsonl\", \"rb\") as f:\n",
    "    uploaded_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "file_id = uploaded_file.id\n",
    "print(\"Uploaded File ID:\", file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03c14991-c412-4932-801c-742ec349ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-FSH8DEYxgpep63RAsJVkVp40\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"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\")   \n",
    "\n",
    "# Start fine-tuning\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-WBomPEap6Dgg6N4hmQ6wn3\", \n",
    "    model=\"gpt-3.5-turbo\",  # Added the required 'model' parameter\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Output the Job ID\n",
    "print(\"Fine-tuning Job ID:\", fine_tune_job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2586a205-db26-4ccc-9d95-22f16ab506dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "1753150098 - Step 1/150: training loss=3.12\n",
      "1753150101 - Step 2/150: training loss=3.13\n",
      "1753150103 - Step 3/150: training loss=3.04\n",
      "Current Status: running\n",
      "1753150103 - Step 3/150: training loss=3.04\n",
      "1753150104 - Step 4/150: training loss=2.90\n",
      "1753150104 - Step 5/150: training loss=2.76\n",
      "1753150106 - Step 6/150: training loss=2.48\n",
      "1753150106 - Step 7/150: training loss=2.17\n",
      "1753150109 - Step 8/150: training loss=1.84\n",
      "1753150112 - Step 9/150: training loss=1.53\n",
      "1753150112 - Step 10/150: training loss=1.30\n",
      "1753150112 - Step 11/150: training loss=1.15\n",
      "1753150115 - Step 12/150: training loss=0.99\n",
      "Current Status: running\n",
      "1753150106 - Step 6/150: training loss=2.48\n",
      "1753150106 - Step 7/150: training loss=2.17\n",
      "1753150109 - Step 8/150: training loss=1.84\n",
      "1753150112 - Step 9/150: training loss=1.53\n",
      "1753150112 - Step 10/150: training loss=1.30\n",
      "1753150112 - Step 11/150: training loss=1.15\n",
      "1753150115 - Step 12/150: training loss=0.99\n",
      "1753150115 - Step 13/150: training loss=0.82\n",
      "1753150118 - Step 14/150: training loss=0.66\n",
      "1753150121 - Step 15/150: training loss=0.52\n",
      "Current Status: running\n",
      "1753150121 - Step 15/150: training loss=0.52\n",
      "1753150121 - Step 16/150: training loss=0.37\n",
      "1753150121 - Step 17/150: training loss=0.25\n",
      "1753150124 - Step 18/150: training loss=0.14\n",
      "1753150124 - Step 19/150: training loss=0.06\n",
      "1753150127 - Step 20/150: training loss=0.03\n",
      "1753150127 - Step 21/150: training loss=0.00\n",
      "1753150130 - Step 22/150: training loss=0.00\n",
      "1753150130 - Step 23/150: training loss=0.00\n",
      "1753150132 - Step 24/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150130 - Step 22/150: training loss=0.00\n",
      "1753150130 - Step 23/150: training loss=0.00\n",
      "1753150132 - Step 24/150: training loss=0.00\n",
      "1753150133 - Step 25/150: training loss=0.00\n",
      "1753150135 - Step 26/150: training loss=0.00\n",
      "1753150135 - Step 27/150: training loss=0.00\n",
      "1753150138 - Step 28/150: training loss=0.00\n",
      "1753150138 - Step 29/150: training loss=0.00\n",
      "1753150141 - Step 30/150: training loss=0.00\n",
      "1753150144 - Step 31/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150144 - Step 31/150: training loss=0.00\n",
      "1753150144 - Step 32/150: training loss=0.00\n",
      "1753150144 - Step 33/150: training loss=0.00\n",
      "1753150147 - Step 34/150: training loss=0.00\n",
      "1753150147 - Step 35/150: training loss=0.00\n",
      "1753150150 - Step 36/150: training loss=0.00\n",
      "1753150150 - Step 37/150: training loss=0.00\n",
      "1753150153 - Step 38/150: training loss=0.00\n",
      "1753150153 - Step 39/150: training loss=0.00\n",
      "1753150155 - Step 40/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150153 - Step 39/150: training loss=0.00\n",
      "1753150155 - Step 40/150: training loss=0.00\n",
      "1753150156 - Step 41/150: training loss=0.00\n",
      "1753150159 - Step 42/150: training loss=0.00\n",
      "1753150159 - Step 43/150: training loss=0.00\n",
      "1753150161 - Step 44/150: training loss=0.00\n",
      "1753150161 - Step 45/150: training loss=0.00\n",
      "1753150164 - Step 46/150: training loss=0.00\n",
      "1753150164 - Step 47/150: training loss=0.00\n",
      "1753150167 - Step 48/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150164 - Step 46/150: training loss=0.00\n",
      "1753150164 - Step 47/150: training loss=0.00\n",
      "1753150167 - Step 48/150: training loss=0.00\n",
      "1753150167 - Step 49/150: training loss=0.00\n",
      "1753150170 - Step 50/150: training loss=0.00\n",
      "1753150173 - Step 51/150: training loss=0.00\n",
      "1753150173 - Step 52/150: training loss=0.00\n",
      "1753150176 - Step 53/150: training loss=0.00\n",
      "1753150176 - Step 54/150: training loss=0.00\n",
      "1753150179 - Step 55/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150170 - Step 50/150: training loss=0.00\n",
      "1753150173 - Step 51/150: training loss=0.00\n",
      "1753150173 - Step 52/150: training loss=0.00\n",
      "1753150176 - Step 53/150: training loss=0.00\n",
      "1753150176 - Step 54/150: training loss=0.00\n",
      "1753150179 - Step 55/150: training loss=0.00\n",
      "1753150179 - Step 56/150: training loss=0.00\n",
      "1753150182 - Step 57/150: training loss=0.00\n",
      "1753150182 - Step 58/150: training loss=0.00\n",
      "1753150185 - Step 59/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150182 - Step 58/150: training loss=0.00\n",
      "1753150185 - Step 59/150: training loss=0.00\n",
      "1753150185 - Step 60/150: training loss=0.00\n",
      "1753150188 - Step 61/150: training loss=0.00\n",
      "1753150188 - Step 62/150: training loss=0.00\n",
      "1753150191 - Step 63/150: training loss=0.00\n",
      "1753150191 - Step 64/150: training loss=0.00\n",
      "1753150193 - Step 65/150: training loss=0.00\n",
      "1753150193 - Step 66/150: training loss=0.00\n",
      "1753150196 - Step 67/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150193 - Step 66/150: training loss=0.00\n",
      "1753150196 - Step 67/150: training loss=0.00\n",
      "1753150196 - Step 68/150: training loss=0.00\n",
      "1753150199 - Step 69/150: training loss=0.00\n",
      "1753150199 - Step 70/150: training loss=0.00\n",
      "1753150202 - Step 71/150: training loss=0.00\n",
      "1753150202 - Step 72/150: training loss=0.00\n",
      "1753150205 - Step 73/150: training loss=0.00\n",
      "1753150205 - Step 74/150: training loss=0.00\n",
      "1753150208 - Step 75/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150205 - Step 73/150: training loss=0.00\n",
      "1753150205 - Step 74/150: training loss=0.00\n",
      "1753150208 - Step 75/150: training loss=0.00\n",
      "1753150208 - Step 76/150: training loss=0.00\n",
      "1753150211 - Step 77/150: training loss=0.00\n",
      "1753150211 - Step 78/150: training loss=0.00\n",
      "1753150213 - Step 79/150: training loss=0.00\n",
      "1753150214 - Step 80/150: training loss=0.00\n",
      "1753150216 - Step 81/150: training loss=0.00\n",
      "1753150219 - Step 82/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150216 - Step 81/150: training loss=0.00\n",
      "1753150219 - Step 82/150: training loss=0.00\n",
      "1753150219 - Step 83/150: training loss=0.00\n",
      "1753150219 - Step 84/150: training loss=0.00\n",
      "1753150222 - Step 85/150: training loss=0.00\n",
      "1753150222 - Step 86/150: training loss=0.00\n",
      "1753150225 - Step 87/150: training loss=0.00\n",
      "1753150228 - Step 88/150: training loss=0.00\n",
      "1753150228 - Step 89/150: training loss=0.00\n",
      "1753150230 - Step 90/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150228 - Step 89/150: training loss=0.00\n",
      "1753150230 - Step 90/150: training loss=0.00\n",
      "1753150231 - Step 91/150: training loss=0.00\n",
      "1753150233 - Step 92/150: training loss=0.00\n",
      "1753150233 - Step 93/150: training loss=0.00\n",
      "1753150236 - Step 94/150: training loss=0.00\n",
      "1753150236 - Step 95/150: training loss=0.00\n",
      "1753150239 - Step 96/150: training loss=0.00\n",
      "1753150239 - Step 97/150: training loss=0.00\n",
      "1753150242 - Step 98/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150233 - Step 93/150: training loss=0.00\n",
      "1753150236 - Step 94/150: training loss=0.00\n",
      "1753150236 - Step 95/150: training loss=0.00\n",
      "1753150239 - Step 96/150: training loss=0.00\n",
      "1753150239 - Step 97/150: training loss=0.00\n",
      "1753150242 - Step 98/150: training loss=0.00\n",
      "1753150242 - Step 99/150: training loss=0.00\n",
      "1753150242 - Step 100/150: training loss=0.00\n",
      "1753150245 - Step 101/150: training loss=0.00\n",
      "1753150248 - Step 102/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150245 - Step 101/150: training loss=0.00\n",
      "1753150248 - Step 102/150: training loss=0.00\n",
      "1753150248 - Step 103/150: training loss=0.00\n",
      "1753150251 - Step 104/150: training loss=0.00\n",
      "1753150251 - Step 105/150: training loss=0.00\n",
      "1753150253 - Step 106/150: training loss=0.00\n",
      "1753150254 - Step 107/150: training loss=0.00\n",
      "1753150256 - Step 108/150: training loss=0.00\n",
      "1753150256 - Step 109/150: training loss=0.00\n",
      "1753150259 - Step 110/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150256 - Step 109/150: training loss=0.00\n",
      "1753150259 - Step 110/150: training loss=0.00\n",
      "1753150262 - Step 111/150: training loss=0.00\n",
      "1753150262 - Step 112/150: training loss=0.00\n",
      "1753150262 - Step 113/150: training loss=0.00\n",
      "1753150265 - Step 114/150: training loss=0.00\n",
      "1753150265 - Step 115/150: training loss=0.00\n",
      "1753150268 - Step 116/150: training loss=0.00\n",
      "1753150268 - Step 117/150: training loss=0.00\n",
      "1753150271 - Step 118/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150268 - Step 116/150: training loss=0.00\n",
      "1753150268 - Step 117/150: training loss=0.00\n",
      "1753150271 - Step 118/150: training loss=0.00\n",
      "1753150271 - Step 119/150: training loss=0.00\n",
      "1753150274 - Step 120/150: training loss=0.00\n",
      "1753150274 - Step 121/150: training loss=0.00\n",
      "1753150276 - Step 122/150: training loss=0.00\n",
      "1753150279 - Step 123/150: training loss=0.00\n",
      "1753150279 - Step 124/150: training loss=0.00\n",
      "1753150282 - Step 125/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150279 - Step 124/150: training loss=0.00\n",
      "1753150282 - Step 125/150: training loss=0.00\n",
      "1753150282 - Step 126/150: training loss=0.00\n",
      "1753150285 - Step 127/150: training loss=0.00\n",
      "1753150285 - Step 128/150: training loss=0.00\n",
      "1753150288 - Step 129/150: training loss=0.00\n",
      "1753150288 - Step 130/150: training loss=0.00\n",
      "1753150291 - Step 131/150: training loss=0.00\n",
      "1753150291 - Step 132/150: training loss=0.00\n",
      "1753150293 - Step 133/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150291 - Step 132/150: training loss=0.00\n",
      "1753150293 - Step 133/150: training loss=0.00\n",
      "1753150293 - Step 134/150: training loss=0.00\n",
      "1753150296 - Step 135/150: training loss=0.00\n",
      "1753150296 - Step 136/150: training loss=0.00\n",
      "1753150299 - Step 137/150: training loss=0.00\n",
      "1753150299 - Step 138/150: training loss=0.00\n",
      "1753150302 - Step 139/150: training loss=0.00\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150296 - Step 136/150: training loss=0.00\n",
      "1753150299 - Step 137/150: training loss=0.00\n",
      "1753150299 - Step 138/150: training loss=0.00\n",
      "1753150302 - Step 139/150: training loss=0.00\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "1753150988 - Moderation checks for snapshot ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc passed.\n",
      "1753150988 - Usage policy evaluations completed, model is now enabled for sampling\n",
      "Current Status: running\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "1753150988 - Moderation checks for snapshot ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc passed.\n",
      "1753150988 - Usage policy evaluations completed, model is now enabled for sampling\n",
      "1753150993 - The job has successfully completed\n",
      "Current Status: succeeded\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Track job events\n",
    "def track_fine_tune_events(job_id):\n",
    "    while True:\n",
    "        # Fixed: Removed 'id=' keyword and passed job_id as positional argument\n",
    "        events = client.fine_tuning.jobs.list_events(job_id, limit=10)\n",
    "        for event in reversed(events.data):\n",
    "            print(f\"{event.created_at} - {event.message}\")\n",
    "        status = client.fine_tuning.jobs.retrieve(job_id).status\n",
    "        print(f\"Current Status: {status}\")\n",
    "        if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            break\n",
    "        sleep(10)  # check every 10 seconds\n",
    "\n",
    "track_fine_tune_events(\"ftjob-FSH8DEYxgpep63RAsJVkVp40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2664e8f6-7525-4759-b6c6-8699534763e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dcd28-43ba-4b74-918e-dfbdcef95f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdd62cc-fb8c-4bbd-86b0-b4cd9ad9c1ce",
   "metadata": {},
   "source": [
    "Test Prompt the fine tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0caee65f-dd83-4017-9785-06aa563f7b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LLMs lack a deep understanding of context and might not be aware of the most recent case law.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Run a test prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that avoids hallucinations and ensures logical consistency.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the risks of relying on LLMs for legal decisions?\"}\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5275a868-55ba-4274-b90e-b98b8065a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Status: succeeded\n",
      "Model Name: ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine-tune job details\n",
    "job_info = client.fine_tuning.jobs.retrieve(\"ftjob-FSH8DEYxgpep63RAsJVkVp40\")\n",
    "print(\"Final Status:\", job_info.status)\n",
    "print(\"Model Name:\", job_info.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d107ca5-5f71-40dd-8e19-62ee4a718c2b",
   "metadata": {},
   "source": [
    "### Thought Process\n",
    "# I aimed to fine-tune a base GPT-3.5 model to minimize hallucinations and improve logical consistency in answers.\n",
    "\n",
    "### Commentary\n",
    "# The process succeeded without formatting or upload issues once the dataset was corrected. The training loss approached zero, indicating good model convergence. OpenAI’s moderation and usage checks passed, confirming acceptable model behavior.\n",
    "\n",
    "### Evaluation\n",
    "#  I tested the fine-tuned model on questions about hallucinations and observed stronger logical clarity and fewer vague answers compared to base GPT-3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df411a-d6fc-45c5-966b-e426777c61cf",
   "metadata": {},
   "source": [
    "print(\"\"\"\n",
    "# Fine-Tuning GPT-3.5 Turbo – Hallucination & Logical Consistency Model\n",
    "\n",
    "## Project Purpose\n",
    "This project aimed to fine-tune OpenAI's `gpt-3.5-turbo` model to reduce hallucinations and improve logical consistency when answering domain-specific questions. The primary focus was on enhancing reliability, factual correctness, and coherence in complex reasoning scenarios such as legal, medical, and policy-related topics.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Preparation\n",
    "I created a dataset of 50 structured training examples in the `chat-completion` format (`.jsonl`). Each example followed OpenAI’s fine-tuning structure:\n",
    "- A system prompt defining assistant behavior\n",
    "- A user question targeting logical traps or hallucination-prone queries\n",
    "- An assistant answer grounded in verified logic or acknowledging uncertainty where applicable\n",
    "\n",
    "This structure was carefully formatted to avoid common fine-tuning pitfalls (e.g., missing \"messages\" key, invalid roles, or broken JSON).\n",
    "\n",
    "---\n",
    "\n",
    "## Upload & API Use\n",
    "Using the OpenAI Python SDK (`openai >= 1.0`), I uploaded the dataset and created a fine-tuning job with the following parameters:\n",
    "\n",
    "- model = \"gpt-3.5-turbo-0125\"\n",
    "- suffix = \"hallucination-logical-v1\"\n",
    "\n",
    "The job completed successfully in ~150 training steps and passed moderation & usage checks.\n",
    "\n",
    "---\n",
    "\n",
    "## Results & Final Model\n",
    "The fine-tuning job finished with a training loss = 0.00 by step 150, indicating excellent convergence.\n",
    "\n",
    "Final Model Name:\n",
    "ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "I tested the model with previously unseen prompts, including:\n",
    "\n",
    "Prompt Example:\n",
    "“What are the risks of relying on LLMs for legal decisions?”\n",
    "\n",
    "Fine-tuned model response:\n",
    "The assistant clearly listed:\n",
    "- Potential for hallucination\n",
    "- Lack of legal accountability\n",
    "- Gaps in legal precedent understanding\n",
    "\n",
    "Compared to the base model, the fine-tuned version offered more structured, cautious, and logically defensible responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Thought Process & Commentary\n",
    "\n",
    "- Initial Challenge: Understanding OpenAI’s new v1.0 API interface was a key learning curve. Earlier versions used deprecated syntax that required migration.\n",
    "- Dataset Design: Balancing variety and structure was crucial. I curated questions with logical ambiguity and ensured clean formatting.\n",
    "- Learning Moment: The job failed initially due to missing \"messages\" keys in the training file. Fixing this emphasized the need for schema validation before upload.\n",
    "- Success Indicator: A training loss near 0 and passing moderation checks validated both model performance and alignment with OpenAI's usage guidelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "- Fine-tuning is powerful – but formatting errors are unforgiving.\n",
    "- Structure matters – keeping role-message clarity is essential.\n",
    "- Monitoring progress via .list_events() and .retrieve() is the best way to diagnose real-time issues.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "This project demonstrates the ability to responsibly fine-tune LLMs for specialized tasks. Future directions may involve increasing data size, applying validation sets, and comparing performance via metrics like BLEU or ROUGE for generation fidelity.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c0cdb-5771-444a-af7f-2b4ef5e46f29",
   "metadata": {},
   "source": [
    "Now using streamlit an GAI interface will be structured and created through streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93bc8b79-0fd4-4e1d-a409-abf4c507247f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 15:06:15.899 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-05 15:06:15.900 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "# hallucination_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key here or in your environment variables\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"🧠 Hallucination Correction Assistant\")\n",
    "st.markdown(\"This app uses a fine-tuned GPT model to correct factual or logical hallucinations in language model outputs.\")\n",
    "\n",
    "# User input prompt\n",
    "user_prompt = st.text_area(\"📝 Enter a potentially hallucinated prompt:\", height=200)\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"Correct It\"):\n",
    "    if user_prompt.strip() == \"\":\n",
    "        st.warning(\"Please enter some text.\")\n",
    "    else:\n",
    "        with st.spinner(\"Analyzing and correcting...\"):\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to detect and correct factual and logical hallucinations.\"},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ]\n",
    "                )\n",
    "                corrected_response = response['choices'][0]['message']['content']\n",
    "                st.success(\"✅ Corrected Response:\")\n",
    "                st.write(corrected_response)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337668fb-c123-4364-9220-9fc5e7ca19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0800de-4d42-4d87-bf6f-a3f34af0f980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d919297-e859-483d-8f63-b60297dd1264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c44e4-1bd2-4e97-ae46-d577c0d2c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eba1d2-e3d3-4d5f-ab5b-399b96638be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1239c2-2862-4754-8083-1ffd9d7d09bc",
   "metadata": {},
   "source": [
    "Connect strea,m list to python and make avaible through pycharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe466b35-56e6-47c1-a314-1e11b94e9f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(75988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.0.0.85:8501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "[IPKernelApp] ERROR | Unable to initialize signal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 663, in initialize\n",
      "    self.init_signal()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 545, in init_signal\n",
      "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
      "  File \"/opt/anaconda3/lib/python3.12/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: signal only works in main thread of the main interpreter\n"
     ]
    }
   ],
   "source": [
    "# To run streamlit from Python, use subprocess\n",
    "import subprocess\n",
    "\n",
    "# Run streamlit command using subprocess\n",
    "subprocess.run([\"streamlit\", \"run\", \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\"])\n",
    "\n",
    "# If you want to include the timestamp as a string, use quotes\n",
    "timestamp = \"2025-8-5 15:06:15.900 Session state does not function when running a script without `streamlit run`\"\n",
    "print(timestamp)\n",
    "\n",
    "# Or if you're trying to log this information\n",
    "import logging\n",
    "logging.info(\"Session state does not function when running a script without `streamlit run`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3caf92d6-bf84-4d4f-8156-7cb81f898ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60203) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60205) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
      "Try 'streamlit run --help' for help.\n",
      "\n",
      "Error: Invalid value: File does not exist: hallucination_app.py\n"
     ]
    }
   ],
   "source": [
    "# Run shell commands in Jupyter Notebook\n",
    "!pip install streamlit openai\n",
    "!streamlit run hallucination_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e3165-7766-4d05-b7ab-384be2326150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
