{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b187d993-d6b6-4fa1-8fef-fe8ddb8d03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample prompt-completion pairs categorized by hallucination type\n",
    "examples = [\n",
    "    # Factual Hallucination\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Who was the first U.S. president to walk on the moon?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No U.S. president has ever walked on the moon.\\n\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Prompt: What year did Albert Einstein win the Nobel Prize in Chemistry?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Albert Einstein won the Nobel Prize in Physics in 1921, not in Chemistry.\\n\"\n",
    "    },\n",
    "\n",
    "    # Logical Chain Consistency\n",
    "    {\n",
    "        \"prompt\": \"Prompt: If all mammals have lungs and whales are mammals, do whales have lungs?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Yes, whales have lungs because they are mammals.\\n\"\n",
    "    },\n",
    "\n",
    "    # Contradiction\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Paris is the capital of France and also the capital of Germany. Is that true?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No, Paris is the capital of France. Berlin is the capital of Germany.\\n\"\n",
    "    },\n",
    "\n",
    "    # Temporal Reasoning\n",
    "    {\n",
    "        \"prompt\": \"Prompt: Was Barack Obama president during the Great Depression?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" No, the Great Depression occurred in the 1930s. Barack Obama was president from 2009 to 2017.\\n\"\n",
    "    },\n",
    "\n",
    "    # False Premise\n",
    "    {\n",
    "        \"prompt\": \"Prompt: How did Cleopatra use airplanes to travel across Egypt?\\n\\n###\\n\\n\",\n",
    "        \"completion\": \" Cleopatra lived over 2,000 years ago, long before airplanes were invented.\\n\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df216a87-d76e-45f9-808e-8d178cd0ac50",
   "metadata": {},
   "source": [
    "Next, examples of Hallucinations will be saved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521091df-54ad-4f96-bded-5b61cf945505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 examples saved to hallucination_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save the list to a JSONL file\n",
    "output_file = \"hallucination_dataset.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in examples:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"{len(examples)} examples saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb85cdee-f9cc-4091-9a61-9272c1b78f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Your file contains 6 prompt-completion pairs. In general, we recommend having at least a few hundred examples. We've found that performance tends to linearly increase for every doubling of the number of examples\n",
      "- All prompts end with suffix `?\\n\\n###\\n\\n`\n",
      "- All prompts start with prefix `Prompt: `\n",
      "- All completions end with suffix `.\\n`\n",
      "\n",
      "No remediations found.\n",
      "\n",
      "You can use your file for fine-tuning:\n",
      "> openai api fine_tunes.create -t \"hallucination_dataset.jsonl\"\n",
      "\n",
      "After youâ€™ve fine-tuned a model, remember that your prompt has to end with the indicator string `?\\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\".\\n\"]` so that the generated texts ends at the expected place.\n",
      "Once your model starts training, it'll approximately take 2.53 minutes to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f hallucination_dataset.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa07738-c906-4140-9fae-ed1a2190b177",
   "metadata": {},
   "source": [
    "    Then we will  install and import an Open AI SDK then connect it to the APi platform by OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c88fc3ab-3bf5-4387-8d7c-ec4de9e075c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "File ID: file-BwytE9Kx1NgdC7hBzuGshe\n",
      "SyncCursorPage[FileObject](data=[FileObject(id='file-BwytE9Kx1NgdC7hBzuGshe', bytes=1107, created_at=1753142756, filename='hallucination_dataset.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None), FileObject(id='file-Qtxcd5isZXdBHTXuSrGEe2', bytes=1107, created_at=1753141190, filename='hallucination_dataset.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)], has_more=False, object='list', first_id='file-BwytE9Kx1NgdC7hBzuGshe', last_id='file-Qtxcd5isZXdBHTXuSrGEe2')\n"
     ]
    }
   ],
   "source": [
    "# Install the updated OpenAI SDK\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import the updated client\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Create the client using your API key\n",
    "client = OpenAI(api_key=\"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\") \n",
    "\n",
    "# Upload your file\n",
    "with open(\"hallucination_dataset.jsonl\", \"rb\") as f:\n",
    "    file_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "print(\"File ID:\", file_response.id)\n",
    "\n",
    "# List all files\n",
    "file_list = client.files.list()\n",
    "print(file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdec09-a989-4389-9f02-8b767bc6e687",
   "metadata": {},
   "source": [
    "Now i will provide the model with examples of catergoerized examples of prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21aa7f6-31a5-42b7-9336-3117efa32a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_dataset_50.jsonl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Categories\n",
    "categories = {\n",
    "    \"Factual Hallucination\": [\n",
    "        (\"Who was the first U.S. president to walk on the moon?\", \" No U.S. president has ever walked on the moon.\\n\"),\n",
    "        (\"What year did Albert Einstein win the Nobel Prize in Chemistry?\", \" Albert Einstein won the Nobel Prize in Physics in 1921, not in Chemistry.\\n\"),\n",
    "        (\"What city is the capital of Atlantis?\", \" Atlantis is a mythical place and does not have a real capital.\\n\"),\n",
    "        (\"Who directed the 1890 movie Titanic?\", \" The movie Titanic was released in 1997 and was directed by James Cameron.\\n\"),\n",
    "        (\"Which planet in our solar system is made of chocolate?\", \" No planet in our solar system is made of chocolate.\\n\")\n",
    "    ],\n",
    "    \"Logical Chain\": [\n",
    "        (\"If all birds can fly and penguins are birds, can penguins fly?\", \" No, penguins are birds but cannot fly.\\n\"),\n",
    "        (\"If dogs are mammals and mammals have hearts, do dogs have hearts?\", \" Yes, dogs have hearts because they are mammals.\\n\"),\n",
    "        (\"If a square has four equal sides and a rectangle has four sides, is every rectangle a square?\", \" No, not every rectangle is a square.\\n\"),\n",
    "        (\"If all apples are fruits and all fruits are edible, are apples edible?\", \" Yes, apples are edible because they are fruits.\\n\"),\n",
    "        (\"If John is taller than Alice and Alice is taller than Bob, who is the tallest?\", \" John is the tallest.\\n\")\n",
    "    ],\n",
    "    \"Contradiction\": [\n",
    "        (\"Water is dry. Is this true?\", \" No, water is not dry.\\n\"),\n",
    "        (\"The sun is cold and dark. Is that accurate?\", \" No, the sun is hot and emits light.\\n\"),\n",
    "        (\"Cats are reptiles. Agree or disagree?\", \" Disagree. Cats are mammals, not reptiles.\\n\"),\n",
    "        (\"Fire is made of ice. Explain.\", \" That is incorrect. Fire and ice are opposite in nature.\\n\"),\n",
    "        (\"Birds have fur. Is that correct?\", \" No, birds have feathers, not fur.\\n\")\n",
    "    ],\n",
    "    \"Temporal Reasoning\": [\n",
    "        (\"Was Barack Obama president during the Civil War?\", \" No, the U.S. Civil War occurred in the 1860s. Barack Obama was president from 2009 to 2017.\\n\"),\n",
    "        (\"Did Leonardo da Vinci tweet his inventions?\", \" No, Twitter did not exist during da Vinci's time in the 15th and 16th centuries.\\n\"),\n",
    "        (\"Did humans walk on Mars in 1969?\", \" No, humans walked on the Moon in 1969, not Mars.\\n\"),\n",
    "        (\"Was the iPhone released before the invention of electricity?\", \" No, electricity predates the iPhone by centuries.\\n\"),\n",
    "        (\"Was Julius Caesar interviewed on television?\", \" No, Julius Caesar lived in ancient Rome, long before television was invented.\\n\")\n",
    "    ],\n",
    "    \"False Premise\": [\n",
    "        (\"How did Cleopatra use airplanes to travel across Egypt?\", \" Cleopatra lived over 2,000 years ago, long before airplanes were invented.\\n\"),\n",
    "        (\"What type of email did Abraham Lincoln use?\", \" Abraham Lincoln lived in the 1800s, before email existed.\\n\"),\n",
    "        (\"How many TikTok followers did Shakespeare have?\", \" Shakespeare lived in the 1500s, long before TikTok was created.\\n\"),\n",
    "        (\"What video games did Napoleon enjoy?\", \" Napoleon lived in the 18th century, long before video games were invented.\\n\"),\n",
    "        (\"How did Martin Luther King Jr. post on Instagram?\", \" Instagram did not exist during Martin Luther King Jr.'s lifetime.\\n\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Expand to 50 examples\n",
    "examples = []\n",
    "for category, qa_pairs in categories.items():\n",
    "    for i in range(10):  # Generate 10 prompts per category\n",
    "        prompt_text, answer_text = random.choice(qa_pairs)\n",
    "        prompt = f\"Prompt: {prompt_text}\\n\\n###\\n\\n\"\n",
    "        completion = answer_text\n",
    "        examples.append({\"prompt\": prompt, \"completion\": completion})\n",
    "\n",
    "# Save to JSONL\n",
    "output_file = \"hallucination_dataset_50.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in examples:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "output_file  # Return path to file for user download\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e72cb2-afeb-448d-9776-c77765f7396c",
   "metadata": {},
   "source": [
    "--Upload json and prepare for for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d71032-98c0-4fa9-8363-a3ac4a151578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Uploaded File ID: file-MaYnataKJjZ2xhQ4cpT35u\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Upload Json\n",
    "with open(\"hallucination_dataset_50.jsonl\", \"rb\") as file:\n",
    "    upload_response = client.files.create(\n",
    "        file=file,\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "file_id = upload_response.id  # Note: using .id instead of [\"id\"]\n",
    "print(\"Uploaded File ID:\", file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd2c29a-3906-46f4-8bf2-26fcc57827fe",
   "metadata": {},
   "source": [
    "next a fine tunig job will be created to train the model on the different catergories to be tested "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9eb13c4e-1cae-4916-b658-baaebb8ffbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Fine-tune Job ID: ftjob-JvW2TQLpItNYQK3pO7lA83dd\n"
     ]
    }
   ],
   "source": [
    "# Make sure the library is installed\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import the client\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the client by passing your API key directly\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Create the fine-tuning job\n",
    "fine_tune_response = client.fine_tuning.jobs.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Retrieve and print the job ID\n",
    "job_id = fine_tune_response.id\n",
    "print(\"Fine-tune Job ID:\", job_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1106ce28-aa1d-4e80-b5a6-3d83dc50e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from openai==0.28) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.11.0)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.97.0\n",
      "    Uninstalling openai-1.97.0:\n",
      "      Successfully uninstalled openai-1.97.0\n",
      "Successfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "597ded81-1221-4295-8081-f0b4eb3ff0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: failed\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Authenticate\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Use your actual fine-tune job ID below\n",
    "job_id = \"ftjob-JvW2TQLpItNYQK3pO7lA83dd\"  \n",
    "\n",
    "# Track the fine-tuning job status\n",
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(\"Job Status:\", response.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40e30d-fcab-4332-8e87-9c988b29cb10",
   "metadata": {},
   "source": [
    "Update syntax due to failed job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "807f1984-126e-481d-91fa-7a2893680484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job failed due to a file format error in the training file. Invalid file format. Example 1 is missing key \"messages\".\n",
      "Validating training file: file-MaYnataKJjZ2xhQ4cpT35u\n",
      "Created fine-tuning job: ftjob-JvW2TQLpItNYQK3pO7lA83dd\n"
     ]
    }
   ],
   "source": [
    "# The correct syntax likely requires passing the job_id directly as a positional argument\n",
    "# or using a different parameter name according to the updated API\n",
    "events = client.fine_tuning.jobs.list_events(\"ftjob-JvW2TQLpItNYQK3pO7lA83dd\", limit=20)\n",
    "# Alternatively, if the API structure has changed:\n",
    "# events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-JvW2TQLpItNYQK3pO7lA83dd\", limit=20)\n",
    "\n",
    "for e in events.data:\n",
    "    print(e.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feb8dc04-fdfd-47b8-ac3b-2dfcec1efc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_chat_50.jsonl'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create 50 varied, correctly structured chat-format examples\n",
    "chat_examples = []\n",
    "for i in range(1, 51):\n",
    "    chat_examples.append({\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that detects hallucinations and ensures logical consistency.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Example {i}: Can I time travel using a microwave and some magnets?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"No, that is a hallucination. Time travel is not possible with current technology, especially not using household items.\"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Save all 50 as a JSONL file\n",
    "file_path_50 = \"hallucination_chat_50.jsonl\"\n",
    "with open(file_path_50, \"w\") as f:\n",
    "    for entry in chat_examples:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "file_path_50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca792cc-d560-41ed-b95d-3108a95f1b94",
   "metadata": {},
   "source": [
    "Re-prep the model for fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "045a89be-8e1a-4e5a-a61c-34141444ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (0.28.0)\n",
      "Collecting openai\n",
      "  Using cached openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Using cached openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.0\n",
      "    Uninstalling openai-0.28.0:\n",
      "      Successfully uninstalled openai-0.28.0\n",
      "Successfully installed openai-1.97.0\n",
      "Uploaded File ID: file-UYsRw5D6f2RjeUUtdshdHj\n"
     ]
    }
   ],
   "source": [
    "# Install the latest OpenAI package (if not already)\n",
    "!pip install --upgrade openai\n",
    "\n",
    "# Import and configure the OpenAI client (v1.x+ syntax)\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Upload your fine-tuning file\n",
    "with open(\"hallucination_chat_50.jsonl\", \"rb\") as f:\n",
    "    upload_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "# Get and display the uploaded file ID\n",
    "file_id = upload_response.id\n",
    "print(\"Uploaded File ID:\", file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba6b9c1c-2ac9-4b47-b2bb-8307405a9d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8966837-0c8f-43f0-bde1-76654c0a6b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-DtRs1pqLScMHccUYif6hacT2\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "# Create fine-tuning job\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-MaYnataKJjZ2xhQ4cpT35u\",  \n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Output the Job ID\n",
    "print(\"Fine-tuning Job ID:\", fine_tune_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b63f67-334c-4c87-b953-716a61a7d888",
   "metadata": {},
   "source": [
    "Next we will track the  perfomance and retest for a succesful test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c9e36ca-85c8-4d0d-8601-0fe355240608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: failed\n"
     ]
    }
   ],
   "source": [
    "# Track job status\n",
    "job_id = \"ftjob-Y8NL2MHx3YKB9xcShJVITb1o\"  \n",
    "\n",
    "job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(\"Job Status:\", job_status.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643cce55-30ac-4208-ba01-eaae6a3b49a7",
   "metadata": {},
   "source": [
    "We will relist the fine tuning events due tp the version of the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f754e87c-c44e-4e24-a49c-6b8fadd1079e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753147028 - The job failed due to a file format error in the training file. Invalid file format. Example 1 is missing key \"messages\".\n",
      "1753147026 - Validating training file: file-MaYnataKJjZ2xhQ4cpT35u\n",
      "1753147026 - Created fine-tuning job: ftjob-Y8NL2MHx3YKB9xcShJVITb1o\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual job ID\n",
    "job_id = \"ftjob-Y8NL2MHx3YKB9xcShJVITb1o\"\n",
    "\n",
    "# Correct way to list events for a fine-tuning job\n",
    "events = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
    "# OR depending on the API version:\n",
    "# events = client.fine_tuning.jobs.list_events(id=job_id)\n",
    "\n",
    "for event in events.data:\n",
    "    # Use dot notation instead of dictionary-style access\n",
    "    # The FineTuningJobEvent object has properties that can be accessed with dot notation\n",
    "    print(f\"{event.created_at} - {event.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c077a57-9555-4dbf-85a8-46fd51fe85bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallucination_chat_50_fixed.jsonl'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Generate 50 properly formatted chat-style examples\n",
    "examples = []\n",
    "for i in range(1, 51):\n",
    "    example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant that avoids hallucinations and ensures logical consistency.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question {i}: What are some ways to prevent AI from hallucinating responses?\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Answer {i}: AI hallucinations can be reduced by grounding the model in verified data, using retrieval-augmented generation, and fine-tuning on domain-specific examples.\"}\n",
    "        ]\n",
    "    }\n",
    "    examples.append(example)\n",
    "\n",
    "# Save as JSONL file\n",
    "jsonl_path = \"hallucination_chat_50_fixed.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "jsonl_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b424459-2774-4ec0-933b-e33a3cf592b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded File ID: file-WBomPEap6Dgg6N4hmQ6wn3\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "# Upload the corrected file\n",
    "with open(\"hallucination_chat_50_fixed.jsonl\", \"rb\") as f:\n",
    "    uploaded_file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "\n",
    "file_id = uploaded_file.id\n",
    "print(\"Uploaded File ID:\", file_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03c14991-c412-4932-801c-742ec349ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning Job ID: ftjob-FSH8DEYxgpep63RAsJVkVp40\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=\"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\")   \n",
    "\n",
    "# Start fine-tuning\n",
    "fine_tune_job = client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-WBomPEap6Dgg6N4hmQ6wn3\", \n",
    "    model=\"gpt-3.5-turbo\",  # Added the required 'model' parameter\n",
    "    suffix=\"hallucination-logical-v1\"\n",
    ")\n",
    "\n",
    "# Output the Job ID\n",
    "print(\"Fine-tuning Job ID:\", fine_tune_job.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2586a205-db26-4ccc-9d95-22f16ab506dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "Current Status: running\n",
      "1753149880 - Created fine-tuning job: ftjob-FSH8DEYxgpep63RAsJVkVp40\n",
      "1753149880 - Validating training file: file-WBomPEap6Dgg6N4hmQ6wn3\n",
      "1753149956 - Files validated, moving job to queued state\n",
      "1753149957 - Fine-tuning job started\n",
      "1753150098 - Step 1/150: training loss=3.12\n",
      "1753150101 - Step 2/150: training loss=3.13\n",
      "1753150103 - Step 3/150: training loss=3.04\n",
      "Current Status: running\n",
      "1753150103 - Step 3/150: training loss=3.04\n",
      "1753150104 - Step 4/150: training loss=2.90\n",
      "1753150104 - Step 5/150: training loss=2.76\n",
      "1753150106 - Step 6/150: training loss=2.48\n",
      "1753150106 - Step 7/150: training loss=2.17\n",
      "1753150109 - Step 8/150: training loss=1.84\n",
      "1753150112 - Step 9/150: training loss=1.53\n",
      "1753150112 - Step 10/150: training loss=1.30\n",
      "1753150112 - Step 11/150: training loss=1.15\n",
      "1753150115 - Step 12/150: training loss=0.99\n",
      "Current Status: running\n",
      "1753150106 - Step 6/150: training loss=2.48\n",
      "1753150106 - Step 7/150: training loss=2.17\n",
      "1753150109 - Step 8/150: training loss=1.84\n",
      "1753150112 - Step 9/150: training loss=1.53\n",
      "1753150112 - Step 10/150: training loss=1.30\n",
      "1753150112 - Step 11/150: training loss=1.15\n",
      "1753150115 - Step 12/150: training loss=0.99\n",
      "1753150115 - Step 13/150: training loss=0.82\n",
      "1753150118 - Step 14/150: training loss=0.66\n",
      "1753150121 - Step 15/150: training loss=0.52\n",
      "Current Status: running\n",
      "1753150121 - Step 15/150: training loss=0.52\n",
      "1753150121 - Step 16/150: training loss=0.37\n",
      "1753150121 - Step 17/150: training loss=0.25\n",
      "1753150124 - Step 18/150: training loss=0.14\n",
      "1753150124 - Step 19/150: training loss=0.06\n",
      "1753150127 - Step 20/150: training loss=0.03\n",
      "1753150127 - Step 21/150: training loss=0.00\n",
      "1753150130 - Step 22/150: training loss=0.00\n",
      "1753150130 - Step 23/150: training loss=0.00\n",
      "1753150132 - Step 24/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150130 - Step 22/150: training loss=0.00\n",
      "1753150130 - Step 23/150: training loss=0.00\n",
      "1753150132 - Step 24/150: training loss=0.00\n",
      "1753150133 - Step 25/150: training loss=0.00\n",
      "1753150135 - Step 26/150: training loss=0.00\n",
      "1753150135 - Step 27/150: training loss=0.00\n",
      "1753150138 - Step 28/150: training loss=0.00\n",
      "1753150138 - Step 29/150: training loss=0.00\n",
      "1753150141 - Step 30/150: training loss=0.00\n",
      "1753150144 - Step 31/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150144 - Step 31/150: training loss=0.00\n",
      "1753150144 - Step 32/150: training loss=0.00\n",
      "1753150144 - Step 33/150: training loss=0.00\n",
      "1753150147 - Step 34/150: training loss=0.00\n",
      "1753150147 - Step 35/150: training loss=0.00\n",
      "1753150150 - Step 36/150: training loss=0.00\n",
      "1753150150 - Step 37/150: training loss=0.00\n",
      "1753150153 - Step 38/150: training loss=0.00\n",
      "1753150153 - Step 39/150: training loss=0.00\n",
      "1753150155 - Step 40/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150153 - Step 39/150: training loss=0.00\n",
      "1753150155 - Step 40/150: training loss=0.00\n",
      "1753150156 - Step 41/150: training loss=0.00\n",
      "1753150159 - Step 42/150: training loss=0.00\n",
      "1753150159 - Step 43/150: training loss=0.00\n",
      "1753150161 - Step 44/150: training loss=0.00\n",
      "1753150161 - Step 45/150: training loss=0.00\n",
      "1753150164 - Step 46/150: training loss=0.00\n",
      "1753150164 - Step 47/150: training loss=0.00\n",
      "1753150167 - Step 48/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150164 - Step 46/150: training loss=0.00\n",
      "1753150164 - Step 47/150: training loss=0.00\n",
      "1753150167 - Step 48/150: training loss=0.00\n",
      "1753150167 - Step 49/150: training loss=0.00\n",
      "1753150170 - Step 50/150: training loss=0.00\n",
      "1753150173 - Step 51/150: training loss=0.00\n",
      "1753150173 - Step 52/150: training loss=0.00\n",
      "1753150176 - Step 53/150: training loss=0.00\n",
      "1753150176 - Step 54/150: training loss=0.00\n",
      "1753150179 - Step 55/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150170 - Step 50/150: training loss=0.00\n",
      "1753150173 - Step 51/150: training loss=0.00\n",
      "1753150173 - Step 52/150: training loss=0.00\n",
      "1753150176 - Step 53/150: training loss=0.00\n",
      "1753150176 - Step 54/150: training loss=0.00\n",
      "1753150179 - Step 55/150: training loss=0.00\n",
      "1753150179 - Step 56/150: training loss=0.00\n",
      "1753150182 - Step 57/150: training loss=0.00\n",
      "1753150182 - Step 58/150: training loss=0.00\n",
      "1753150185 - Step 59/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150182 - Step 58/150: training loss=0.00\n",
      "1753150185 - Step 59/150: training loss=0.00\n",
      "1753150185 - Step 60/150: training loss=0.00\n",
      "1753150188 - Step 61/150: training loss=0.00\n",
      "1753150188 - Step 62/150: training loss=0.00\n",
      "1753150191 - Step 63/150: training loss=0.00\n",
      "1753150191 - Step 64/150: training loss=0.00\n",
      "1753150193 - Step 65/150: training loss=0.00\n",
      "1753150193 - Step 66/150: training loss=0.00\n",
      "1753150196 - Step 67/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150193 - Step 66/150: training loss=0.00\n",
      "1753150196 - Step 67/150: training loss=0.00\n",
      "1753150196 - Step 68/150: training loss=0.00\n",
      "1753150199 - Step 69/150: training loss=0.00\n",
      "1753150199 - Step 70/150: training loss=0.00\n",
      "1753150202 - Step 71/150: training loss=0.00\n",
      "1753150202 - Step 72/150: training loss=0.00\n",
      "1753150205 - Step 73/150: training loss=0.00\n",
      "1753150205 - Step 74/150: training loss=0.00\n",
      "1753150208 - Step 75/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150205 - Step 73/150: training loss=0.00\n",
      "1753150205 - Step 74/150: training loss=0.00\n",
      "1753150208 - Step 75/150: training loss=0.00\n",
      "1753150208 - Step 76/150: training loss=0.00\n",
      "1753150211 - Step 77/150: training loss=0.00\n",
      "1753150211 - Step 78/150: training loss=0.00\n",
      "1753150213 - Step 79/150: training loss=0.00\n",
      "1753150214 - Step 80/150: training loss=0.00\n",
      "1753150216 - Step 81/150: training loss=0.00\n",
      "1753150219 - Step 82/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150216 - Step 81/150: training loss=0.00\n",
      "1753150219 - Step 82/150: training loss=0.00\n",
      "1753150219 - Step 83/150: training loss=0.00\n",
      "1753150219 - Step 84/150: training loss=0.00\n",
      "1753150222 - Step 85/150: training loss=0.00\n",
      "1753150222 - Step 86/150: training loss=0.00\n",
      "1753150225 - Step 87/150: training loss=0.00\n",
      "1753150228 - Step 88/150: training loss=0.00\n",
      "1753150228 - Step 89/150: training loss=0.00\n",
      "1753150230 - Step 90/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150228 - Step 89/150: training loss=0.00\n",
      "1753150230 - Step 90/150: training loss=0.00\n",
      "1753150231 - Step 91/150: training loss=0.00\n",
      "1753150233 - Step 92/150: training loss=0.00\n",
      "1753150233 - Step 93/150: training loss=0.00\n",
      "1753150236 - Step 94/150: training loss=0.00\n",
      "1753150236 - Step 95/150: training loss=0.00\n",
      "1753150239 - Step 96/150: training loss=0.00\n",
      "1753150239 - Step 97/150: training loss=0.00\n",
      "1753150242 - Step 98/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150233 - Step 93/150: training loss=0.00\n",
      "1753150236 - Step 94/150: training loss=0.00\n",
      "1753150236 - Step 95/150: training loss=0.00\n",
      "1753150239 - Step 96/150: training loss=0.00\n",
      "1753150239 - Step 97/150: training loss=0.00\n",
      "1753150242 - Step 98/150: training loss=0.00\n",
      "1753150242 - Step 99/150: training loss=0.00\n",
      "1753150242 - Step 100/150: training loss=0.00\n",
      "1753150245 - Step 101/150: training loss=0.00\n",
      "1753150248 - Step 102/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150245 - Step 101/150: training loss=0.00\n",
      "1753150248 - Step 102/150: training loss=0.00\n",
      "1753150248 - Step 103/150: training loss=0.00\n",
      "1753150251 - Step 104/150: training loss=0.00\n",
      "1753150251 - Step 105/150: training loss=0.00\n",
      "1753150253 - Step 106/150: training loss=0.00\n",
      "1753150254 - Step 107/150: training loss=0.00\n",
      "1753150256 - Step 108/150: training loss=0.00\n",
      "1753150256 - Step 109/150: training loss=0.00\n",
      "1753150259 - Step 110/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150256 - Step 109/150: training loss=0.00\n",
      "1753150259 - Step 110/150: training loss=0.00\n",
      "1753150262 - Step 111/150: training loss=0.00\n",
      "1753150262 - Step 112/150: training loss=0.00\n",
      "1753150262 - Step 113/150: training loss=0.00\n",
      "1753150265 - Step 114/150: training loss=0.00\n",
      "1753150265 - Step 115/150: training loss=0.00\n",
      "1753150268 - Step 116/150: training loss=0.00\n",
      "1753150268 - Step 117/150: training loss=0.00\n",
      "1753150271 - Step 118/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150268 - Step 116/150: training loss=0.00\n",
      "1753150268 - Step 117/150: training loss=0.00\n",
      "1753150271 - Step 118/150: training loss=0.00\n",
      "1753150271 - Step 119/150: training loss=0.00\n",
      "1753150274 - Step 120/150: training loss=0.00\n",
      "1753150274 - Step 121/150: training loss=0.00\n",
      "1753150276 - Step 122/150: training loss=0.00\n",
      "1753150279 - Step 123/150: training loss=0.00\n",
      "1753150279 - Step 124/150: training loss=0.00\n",
      "1753150282 - Step 125/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150279 - Step 124/150: training loss=0.00\n",
      "1753150282 - Step 125/150: training loss=0.00\n",
      "1753150282 - Step 126/150: training loss=0.00\n",
      "1753150285 - Step 127/150: training loss=0.00\n",
      "1753150285 - Step 128/150: training loss=0.00\n",
      "1753150288 - Step 129/150: training loss=0.00\n",
      "1753150288 - Step 130/150: training loss=0.00\n",
      "1753150291 - Step 131/150: training loss=0.00\n",
      "1753150291 - Step 132/150: training loss=0.00\n",
      "1753150293 - Step 133/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150291 - Step 132/150: training loss=0.00\n",
      "1753150293 - Step 133/150: training loss=0.00\n",
      "1753150293 - Step 134/150: training loss=0.00\n",
      "1753150296 - Step 135/150: training loss=0.00\n",
      "1753150296 - Step 136/150: training loss=0.00\n",
      "1753150299 - Step 137/150: training loss=0.00\n",
      "1753150299 - Step 138/150: training loss=0.00\n",
      "1753150302 - Step 139/150: training loss=0.00\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150296 - Step 136/150: training loss=0.00\n",
      "1753150299 - Step 137/150: training loss=0.00\n",
      "1753150299 - Step 138/150: training loss=0.00\n",
      "1753150302 - Step 139/150: training loss=0.00\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150302 - Step 140/150: training loss=0.00\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150305 - Step 141/150: training loss=0.00\n",
      "1753150305 - Step 142/150: training loss=0.00\n",
      "1753150308 - Step 143/150: training loss=0.00\n",
      "1753150308 - Step 144/150: training loss=0.00\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150311 - Step 145/150: training loss=0.00\n",
      "1753150311 - Step 146/150: training loss=0.00\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "Current Status: running\n",
      "1753150314 - Step 147/150: training loss=0.00\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "1753150988 - Moderation checks for snapshot ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc passed.\n",
      "1753150988 - Usage policy evaluations completed, model is now enabled for sampling\n",
      "Current Status: running\n",
      "1753150314 - Step 148/150: training loss=0.00\n",
      "1753150316 - Step 149/150: training loss=0.00\n",
      "1753150317 - Step 150/150: training loss=0.00\n",
      "1753150348 - Checkpoint created at step 50\n",
      "1753150348 - Checkpoint created at step 100\n",
      "1753150348 - New fine-tuned model created\n",
      "1753150348 - Evaluating model against our usage policies\n",
      "1753150988 - Moderation checks for snapshot ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc passed.\n",
      "1753150988 - Usage policy evaluations completed, model is now enabled for sampling\n",
      "1753150993 - The job has successfully completed\n",
      "Current Status: succeeded\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "# Track job events\n",
    "def track_fine_tune_events(job_id):\n",
    "    while True:\n",
    "        # Fixed: Removed 'id=' keyword and passed job_id as positional argument\n",
    "        events = client.fine_tuning.jobs.list_events(job_id, limit=10)\n",
    "        for event in reversed(events.data):\n",
    "            print(f\"{event.created_at} - {event.message}\")\n",
    "        status = client.fine_tuning.jobs.retrieve(job_id).status\n",
    "        print(f\"Current Status: {status}\")\n",
    "        if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "            break\n",
    "        sleep(10)  # check every 10 seconds\n",
    "\n",
    "track_fine_tune_events(\"ftjob-FSH8DEYxgpep63RAsJVkVp40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2664e8f6-7525-4759-b6c6-8699534763e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dcd28-43ba-4b74-918e-dfbdcef95f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdd62cc-fb8c-4bbd-86b0-b4cd9ad9c1ce",
   "metadata": {},
   "source": [
    "Test Prompt the fine tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0caee65f-dd83-4017-9785-06aa563f7b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: LLMs lack a deep understanding of context and might not be aware of the most recent case law.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "client = OpenAI(api_key=\"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\")  \n",
    "\n",
    "# Run a test prompt\n",
    "response = client.chat.completions.create(\n",
    "    model=\"ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that avoids hallucinations and ensures logical consistency.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the risks of relying on LLMs for legal decisions?\"}\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5275a868-55ba-4274-b90e-b98b8065a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Status: succeeded\n",
      "Model Name: ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\n"
     ]
    }
   ],
   "source": [
    "# Retrieve fine-tune job details\n",
    "job_info = client.fine_tuning.jobs.retrieve(\"ftjob-FSH8DEYxgpep63RAsJVkVp40\")\n",
    "print(\"Final Status:\", job_info.status)\n",
    "print(\"Model Name:\", job_info.fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d107ca5-5f71-40dd-8e19-62ee4a718c2b",
   "metadata": {},
   "source": [
    "### Thought Process\n",
    "# I aimed to fine-tune a base GPT-3.5 model to minimize hallucinations and improve logical consistency in answers.\n",
    "\n",
    "### Commentary\n",
    "# The process succeeded without formatting or upload issues once the dataset was corrected. The training loss approached zero, indicating good model convergence. OpenAIâ€™s moderation and usage checks passed, confirming acceptable model behavior.\n",
    "\n",
    "### Evaluation\n",
    "#  I tested the fine-tuned model on questions about hallucinations and observed stronger logical clarity and fewer vague answers compared to base GPT-3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df411a-d6fc-45c5-966b-e426777c61cf",
   "metadata": {},
   "source": [
    "print(\"\"\"\n",
    "# Fine-Tuning GPT-3.5 Turbo â€“ Hallucination & Logical Consistency Model\n",
    "\n",
    "## Project Purpose\n",
    "This project aimed to fine-tune OpenAI's `gpt-3.5-turbo` model to reduce hallucinations and improve logical consistency when answering domain-specific questions. The primary focus was on enhancing reliability, factual correctness, and coherence in complex reasoning scenarios such as legal, medical, and policy-related topics.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Preparation\n",
    "I created a dataset of 50 structured training examples in the `chat-completion` format (`.jsonl`). Each example followed OpenAIâ€™s fine-tuning structure:\n",
    "- A system prompt defining assistant behavior\n",
    "- A user question targeting logical traps or hallucination-prone queries\n",
    "- An assistant answer grounded in verified logic or acknowledging uncertainty where applicable\n",
    "\n",
    "This structure was carefully formatted to avoid common fine-tuning pitfalls (e.g., missing \"messages\" key, invalid roles, or broken JSON).\n",
    "\n",
    "---\n",
    "\n",
    "## Upload & API Use\n",
    "Using the OpenAI Python SDK (`openai >= 1.0`), I uploaded the dataset and created a fine-tuning job with the following parameters:\n",
    "\n",
    "- model = \"gpt-3.5-turbo-0125\"\n",
    "- suffix = \"hallucination-logical-v1\"\n",
    "\n",
    "The job completed successfully in ~150 training steps and passed moderation & usage checks.\n",
    "\n",
    "---\n",
    "\n",
    "## Results & Final Model\n",
    "The fine-tuning job finished with a training loss = 0.00 by step 150, indicating excellent convergence.\n",
    "\n",
    "Final Model Name:\n",
    "ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\n",
    "\n",
    "---\n",
    "\n",
    "## Model Evaluation\n",
    "I tested the model with previously unseen prompts, including:\n",
    "\n",
    "Prompt Example:\n",
    "â€œWhat are the risks of relying on LLMs for legal decisions?â€\n",
    "\n",
    "Fine-tuned model response:\n",
    "The assistant clearly listed:\n",
    "- Potential for hallucination\n",
    "- Lack of legal accountability\n",
    "- Gaps in legal precedent understanding\n",
    "\n",
    "Compared to the base model, the fine-tuned version offered more structured, cautious, and logically defensible responses.\n",
    "\n",
    "---\n",
    "\n",
    "## Thought Process & Commentary\n",
    "\n",
    "- Initial Challenge: Understanding OpenAIâ€™s new v1.0 API interface was a key learning curve. Earlier versions used deprecated syntax that required migration.\n",
    "- Dataset Design: Balancing variety and structure was crucial. I curated questions with logical ambiguity and ensured clean formatting.\n",
    "- Learning Moment: The job failed initially due to missing \"messages\" keys in the training file. Fixing this emphasized the need for schema validation before upload.\n",
    "- Success Indicator: A training loss near 0 and passing moderation checks validated both model performance and alignment with OpenAI's usage guidelines.\n",
    "\n",
    "---\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "- Fine-tuning is powerful â€“ but formatting errors are unforgiving.\n",
    "- Structure matters â€“ keeping role-message clarity is essential.\n",
    "- Monitoring progress via .list_events() and .retrieve() is the best way to diagnose real-time issues.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Notes\n",
    "This project demonstrates the ability to responsibly fine-tune LLMs for specialized tasks. Future directions may involve increasing data size, applying validation sets, and comparing performance via metrics like BLEU or ROUGE for generation fidelity.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c0cdb-5771-444a-af7f-2b4ef5e46f29",
   "metadata": {},
   "source": [
    "Now using streamlit an GAI interface will be structured and created through streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93bc8b79-0fd4-4e1d-a409-abf4c507247f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 15:06:15.899 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-05 15:06:15.900 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "# hallucination_app.py\n",
    "\n",
    "import streamlit as st\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key here or in your environment variables\n",
    "openai.api_key = os.getenv(\"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\")\n",
    "\n",
    "# Title and description\n",
    "st.title(\"ðŸ§  Hallucination Correction Assistant\")\n",
    "st.markdown(\"This app uses a fine-tuned GPT model to correct factual or logical hallucinations in language model outputs.\")\n",
    "\n",
    "# User input prompt\n",
    "user_prompt = st.text_area(\"ðŸ“ Enter a potentially hallucinated prompt:\", height=200)\n",
    "\n",
    "# Submit button\n",
    "if st.button(\"Correct It\"):\n",
    "    if user_prompt.strip() == \"\":\n",
    "        st.warning(\"Please enter some text.\")\n",
    "    else:\n",
    "        with st.spinner(\"Analyzing and correcting...\"):\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"ft:gpt-3.5-turbo-0125:dares-apis:hallucination-logical-v1:BvwmRqwc\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant trained to detect and correct factual and logical hallucinations.\"},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt}\n",
    "                    ]\n",
    "                )\n",
    "                corrected_response = response['choices'][0]['message']['content']\n",
    "                st.success(\"âœ… Corrected Response:\")\n",
    "                st.write(corrected_response)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337668fb-c123-4364-9220-9fc5e7ca19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0800de-4d42-4d87-bf6f-a3f34af0f980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d919297-e859-483d-8f63-b60297dd1264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c44e4-1bd2-4e97-ae46-d577c0d2c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eba1d2-e3d3-4d5f-ab5b-399b96638be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1239c2-2862-4754-8083-1ffd9d7d09bc",
   "metadata": {},
   "source": [
    "Connect strea,m list to python and make avaible through pycharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe466b35-56e6-47c1-a314-1e11b94e9f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(75988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.0.0.85:8501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "[IPKernelApp] ERROR | Unable to initialize signal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 663, in initialize\n",
      "    self.init_signal()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 545, in init_signal\n",
      "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
      "  File \"/opt/anaconda3/lib/python3.12/signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: signal only works in main thread of the main interpreter\n"
     ]
    }
   ],
   "source": [
    "# To run streamlit from Python, use subprocess\n",
    "import subprocess\n",
    "\n",
    "# Run streamlit command using subprocess\n",
    "subprocess.run([\"streamlit\", \"run\", \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\"])\n",
    "\n",
    "# If you want to include the timestamp as a string, use quotes\n",
    "timestamp = \"2025-8-5 15:06:15.900 Session state does not function when running a script without `streamlit run`\"\n",
    "print(timestamp)\n",
    "\n",
    "# Or if you're trying to log this information\n",
    "import logging\n",
    "logging.info(\"Session state does not function when running a script without `streamlit run`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3caf92d6-bf84-4d4f-8156-7cb81f898ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60203) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.97.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (24.1)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(60205) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
      "Try 'streamlit run --help' for help.\n",
      "\n",
      "Error: Invalid value: File does not exist: hallucination_app.py\n"
     ]
    }
   ],
   "source": [
    "# Run shell commands in Jupyter Notebook\n",
    "!pip install streamlit openai\n",
    "!streamlit run hallucination_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e3165-7766-4d05-b7ab-384be2326150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
